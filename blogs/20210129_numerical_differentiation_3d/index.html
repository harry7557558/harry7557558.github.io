<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <title>{%TITLE%}</title>
    <link rel="icon" href="https://harry7557558.github.io/logo.png" />

    <style>
        body {
            background-color: #555;
        }

        #container {
            margin: 40px auto;
            width: 85%;
            box-shadow: 0px 0px 5px rgba(0,0,0,0.6);
            padding: 3% 7% 10% 7%;
            max-width: 900px;
            background-color: #ffffe0;
            border-radius: 12px;
        }

        @media only screen and (max-width: 900px) {
            #container {
                margin: 0;
                box-shadow: none;
                width: 90%;
                padding: 5%;
            }
        }

        #markdown {
            margin: 0;
            font-family: Georgia, 'Times New Roman';
            width: 100%;
            line-height: 1.5;
            color: #222;
        }

            #markdown h1, #markdown h2, #markdown h3, #markdown h4, #markdown h5, #markdown h6 {
                margin-top: 2.5em;
                margin-bottom: 2em;
                font-family: Arial;
                color: #222;
            }

            #markdown p {
                font-size: 1em;
                margin: 1.5em 0em;
            }

            #markdown a {
                color: blue;
                text-decoration: none;
            }

                #markdown a:hover {
                    color: brown;
                    text-decoration: underline;
                }

            #markdown em, #markdown i {
                font-style: italic;
            }

            #markdown strong {
                font-weight: bold;
            }

            #markdown blockquote {
                margin: 0px;
                border-left: 5px solid rgba(128,128,128,0.5);
                padding: 1px 20px;
                background-color: rgba(192,192,192,0.1);
                color: #555;
            }

            #markdown code {
                background-color: rgba(232,232,240,0.5);
                padding: 0.2em;
                border-radius: 0.2em;
            }

            #markdown pre {
                background-color: rgba(232,232,240,0.5);
                padding: 1em;
                border-radius: 0.5em;
                border: 1px solid rgba(180,180,180,0.5);
            }

                #markdown pre code {
                    background-color: transparent;
                    padding: 0em;
                }

        ::selection {
            background-color: lightblue;
        }
    </style>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css" integrity="sha384-qCEsSYDSH0x5I45nNW4oXemORUZnYFtPy/FqB/OjqxabTMW5HVaaH9USK4fN3goV" crossorigin="anonymous">

</head>
<body>
    <div id="container">
        <div id="markdown">
            <h2>Numerical derivative in one dimension</h2>
<p>Numerical differentiation is a basic topic for many applications. When the analytical derivative/gradient of a function is not easy to obtain, they may be evaluated numerically.</p>
<p>Also, when a continuous function/field is defined by a set of discrete sample points, the its numerical gradient needs to be interpolated.</p>
<p>You may know these formulas in one dimension:</p>
<p>$$f'(x) = &#92;dfrac{f(x+h)-f(x)}{h}+O(h) = &#92;dfrac{f(x+h)-f(x-h)}{2h}+O(h^2)$$</p>
<p>$$f''(x) = &#92;dfrac{f(x+h)+f(x-h)-2f(x)}{h^2}+O(h^2)$$</p>
<p>The accuracy can be analyzed using Taylor expansion. For example, to analyze the accuracy of $$f'(x)&#92;approx&#92;frac{f(x+h)-f(x)}{h}$$, one can apply a Taylor expansion to $$f(x+h)$$ and simplify equations:</p>
<p>$$f(x+h)=f(x)+h f'(x)+&#92;dfrac{h^2}{2} f''(x)$$</p>
<p>$$&#92;dfrac{f(x+h)-f(x)}{h}=&#92;dfrac{hf'(x)+&#92;frac{h^2}{2}f''(x)}{h}=f'(x)+&#92;dfrac{h}{2}f''(x)=f'(x)+O(h)$$</p>
<p>So we can proof the error of this method (technically called forward differentiation) is first order. As $$h$$ goes smaller, the error is approximately propotional to $$h$$. Using a similar method, one can analyze the accuracy of other differentiation methods.</p>
<p>Since a too small $$h$$ can cause too much floating point inaccuracy, we can't make $$h$$ as small as possible. We need a method that involves higher order of error. using this method $$f'(x)=&#92;frac{f(x+h)-f(x-h)}{2h}+O(h^2)$$ (technically called central difference), the error is propotional to the square of differentiation step. A step of $$10^{-3}$$ can achieve an accuracy of $$10^{-6}$$, which is enough for calculations using single precision floating point numbers.</p>
<p>Taylor expansion can also be used to obtain numerical differentiation methods. For example, suppose we already have samples $$y&#95;1=f(x+h&#95;1)$$, $$y&#95;2=f(x+h&#95;2)$$, and $$y=f(x)$$, and we need to approximate $$f'(x)$$ and achieve $$O(h^2)$$ error. We let $$f'(x)&#92;approx a&#95;1 y&#95;1 + a&#95;2 y&#95;2 + a y$$, expand $$y&#95;1,y&#95;2,y$$ to the second order term and try to find $$a&#95;1,a&#95;2,a$$ so the expression equals to $$f'(x)$$. I got that $$a&#95;1$$ and $$a&#95;2$$ can be solved by solving $$&#92;begin{bmatrix}h&#95;1^2&amp;h&#95;2^2&#92;&#92;h&#95;1&amp;h&#95;2&#92;end{bmatrix}&#92;begin{bmatrix}a&#95;1&#92;&#92;a&#95;2&#92;end{bmatrix}=&#92;begin{bmatrix}0&#92;&#92;1&#92;end{bmatrix}$$ and $$a=-(a&#95;1+a&#95;2)$$. I used this method to approximate the tangent of a parametric curve defined by discrete sample points.</p>
<h2>Numerical gradient: forward differentiation and central difference</h2>
<p>Calculating numerical gradient in higher dimensions seems to be straightforward. By definition, $$&#92;nabla f = &#92;begin{pmatrix}&#92;frac{&#92;partial f}{&#92;partial x&#95;1}&amp;&#92;frac{&#92;partial f}{&#92;partial x&#95;2}&amp;&#92;cdots&amp;&#92;frac{&#92;partial f}{&#92;partial x&#95;d}&#92;end{pmatrix}^T$$. Calculating the numerical derivative in each dimension using the one dimensional method and combine the components together one can get the numerical gradient in high dimension.</p>
<p>We have seen the forward differentiation in one dimension:</p>
<p>$$&#92;dfrac{&#92;partial f}{&#92;partial x} &#92;approx &#92;dfrac{f(x+h)-f(x)}{h}+&#92;dfrac{h}{2}f''(x)$$</p>
<p>Apply it to high dimension:</p>
<p>$$&#92;nabla f&#92;approx&#92;dfrac{1}{h}&#92;begin{bmatrix}f(&#92;mathbf{x}+h&#92;mathbf{e}&#95;i)-f(&#92;mathbf{x})&#92;text{&#92; for&#92; } &#92;mathbf{e}&#95;i &#92;text{&#92; in each dimension}&#92;end{bmatrix}^T$$</p>
<p>The error is approximately $$&#92;dfrac{h}{2}&#92;begin{pmatrix}&#92;dfrac{&#92;partial^2 f}{&#92;partial x&#95;1^2}&amp;&#92;dfrac{&#92;partial^2 f}{&#92;partial x&#95;2^2}&amp;&#92;cdots&amp;&#92;dfrac{&#92;partial^2 f}{&#92;partial x&#95;d^2}&#92;end{pmatrix}^T$$.</p>
<p>Using central difference:</p>
<p>$$&#92;nabla f&#92;approx&#92;dfrac{1}{2h}&#92;begin{bmatrix}f(&#92;mathbf{x}+h&#92;mathbf{e}&#95;i)-f(&#92;mathbf{x}-h&#92;mathbf{e}&#95;i)&#92;text{&#92; for&#92; } &#92;mathbf{e}&#95;i &#92;text{&#92; in each dimension}&#92;end{bmatrix}^T$$</p>
<p>In each dimension, the error is approximately $$&#92;dfrac{h^2}{6}&#92;dfrac{&#92;partial^3 f}{&#92;partial x&#95;i^3}$$.</p>
<p>The central difference has a higher order of accuracy than forward differentiation. However, note that forward differentiation requires $$N+1$$ samples for a $$N$$ dimensional function ($$N$$ samples if $$f(&#92;mathbf{x})$$ is already evaluated), and central difference requires $$2N$$ samples. If the function we needs to differentiate is very expensive to evaluate, the central difference method requires more calculation cost.</p>
<h2>Tetrahedron method: more accurate or not?</h2>
<p>I read the tetrahedron method for calculating the numerical gradient in three dimensions from <a href="https://www.iquilezles.org/www/articles/normalsSDF/normalsSDF.htm">this article</a> written by Inigo Quilez. The method is based on central difference, but it uses only four samples instead of six.</p>
<p>As its name states, the four samples are the vertices of a tetrahedron centered at the point that the gradient needs to be evaluated. The technique is as following:</p>
<p>$$&#92;nabla f(p)=&#92;begin{pmatrix}&#92;dfrac{&#92;partial f}{&#92;partial x}&amp;&#92;dfrac{&#92;partial f}{&#92;partial y}&amp;&#92;dfrac{&#92;partial f}{&#92;partial z}&#92;end{pmatrix} &#92;approx &#92;dfrac{1}{4h}&#92;begin{bmatrix}1&amp;1&amp;-1&amp;-1&#92;&#92;1&amp;-1&amp;1&amp;-1&#92;&#92;1&amp;-1&amp;-1&amp;1&#92;end{bmatrix}&#92;begin{bmatrix}f(p+(h,h,h))&#92;&#92;f(p+(h,-h,-h))&#92;&#92;f(p+(-h,h,-h))&#92;&#92;f(p+(-h,-h,h))&#92;end{bmatrix}$$</p>
<p>Where $$f$$ is a three-dimensional field that its gradient is to be calculated at $$p$$, and $$h$$ is the differentiation step.</p>
<p>Here is a visual of the four sample points:</p>
<p><img alt="tetrahedron.png" src="tetrahedron.png" /></p>
<p>To show if this method works, I first see the Taylor expansion of the four sample points, and try to see if the coefficient of its $$f(p)$$ term is $$0$$ and $$f'(p)$$ term is $$1$$.</p>
<p>First see the Taylor expansion of a function in three dimension:</p>
<p>$$f(p+&#92;mathbf{h})=f(p)+&#92;mathbf{h}^T&#92;dfrac{&#92;partial f}{&#92;partial p}+&#92;dfrac{1}{2}&#92;mathbf{h}^T&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;mathbf{h}+O(&#92;mathbf{h}^3)$$</p>
<p>A vector of the $$f(p)$$ term of the four sample points would be $$&#92;begin{bmatrix}f(p)&amp;f(p)&amp;f(p)&amp;f(p)&#92;end{bmatrix}^T$$. Obviously:</p>
<p>$$&#92;dfrac{1}{4h}&#92;begin{bmatrix}1&amp;1&amp;-1&amp;-1&#92;&#92;1&amp;-1&amp;1&amp;-1&#92;&#92;1&amp;-1&amp;-1&amp;1&#92;end{bmatrix}&#92;begin{bmatrix}f(p)&#92;&#92;f(p)&#92;&#92;f(p)&#92;&#92;f(p)&#92;end{bmatrix}=&#92;mathbf{0}$$</p>
<p>A vector of the $$f'(p)$$ term of the four sample points would be $$&#92;begin{bmatrix}&#92;mathbf{h}&#95;1&amp;&#92;mathbf{h}&#95;2&amp;&#92;mathbf{h}&#95;3&amp;&#92;mathbf{h}&#95;4&#92;end{bmatrix}^T&#92;nabla p$$. By calculating the following matrix product:</p>
<p>$$&#92;dfrac{1}{4h}&#92;begin{bmatrix}1&amp;1&amp;-1&amp;-1&#92;&#92;1&amp;-1&amp;1&amp;-1&#92;&#92;1&amp;-1&amp;-1&amp;1&#92;end{bmatrix}&#92;begin{bmatrix}h&amp;h&amp;h&#92;&#92;h&amp;-h&amp;-h&#92;&#92;-h&amp;h&amp;-h&#92;&#92;-h&amp;-h&amp;h&#92;end{bmatrix}&#92;nabla p=&#92;dfrac{1}{4h}&#92;begin{bmatrix}4h&amp;0&amp;0&#92;&#92;0&amp;4h&amp;0&#92;&#92;0&amp;0&amp;4h&#92;end{bmatrix}&#92;nabla p=&#92;nabla p$$</p>
<p>And here we shows this method works.</p>
<p>Since the gradient is evaluated at the middle of the sample points, one may intuitive think this method has second order accuracy that its error is $$O(h^2)$$. Is it true? Lets have a look at its second order term.</p>
<p>The second derivative matrix is: $$&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}=&#92;begin{bmatrix}f''&#95;{xx}&amp;f''&#95;{xy}&amp;f''&#95;{xz}&#92;&#92;f''&#95;{xy}&amp;f''&#95;{yy}&amp;f''&#95;{yz}&#92;&#92;f''&#95;{xz}&amp;f''&#95;{yz}&amp;f''&#95;{zz}&#92;end{bmatrix}$$</p>
<p>Let $$.$$ denote the matrix dot product (or scalar product, sum of product of correstponding elements). For the first sample, the second-order term is:</p>
<p>$$&#92;dfrac{1}{2}&#92;mathbf{h}&#95;1^T&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;mathbf{h}&#95;1=&#92;dfrac{1}{2}(&#92;mathbf{h}&#95;1&#92;mathbf{h}&#95;1^T)&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}=&#92;dfrac{1}{2}&#92;begin{bmatrix}h&#92;&#92;h&#92;&#92;h&#92;end{bmatrix}&#92;begin{bmatrix}h&amp;h&amp;h&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}=&#92;dfrac{h^2}{2}&#92;begin{bmatrix}1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}$$</p>
<p>For the second, third, and fourth samples, the second-order terms are:</p>
<p>$$&#92;dfrac{1}{2}&#92;begin{bmatrix}h&#92;&#92;-h&#92;&#92;-h&#92;end{bmatrix}&#92;begin{bmatrix}h&amp;-h&amp;-h&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}=&#92;dfrac{h^2}{2}&#92;begin{bmatrix}1&amp;-1&amp;-1&#92;&#92;-1&amp;1&amp;1&#92;&#92;-1&amp;1&amp;1&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}$$</p>
<p>$$&#92;dfrac{1}{2}&#92;begin{bmatrix}-h&#92;&#92;h&#92;&#92;-h&#92;end{bmatrix}&#92;begin{bmatrix}-h&amp;h&amp;-h&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}=&#92;dfrac{h^2}{2}&#92;begin{bmatrix}1&amp;-1&amp;1&#92;&#92;-1&amp;1&amp;-1&#92;&#92;1&amp;-1&amp;1&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}$$</p>
<p>$$&#92;dfrac{1}{2}&#92;begin{bmatrix}-h&#92;&#92;-h&#92;&#92;h&#92;end{bmatrix}&#92;begin{bmatrix}-h&amp;-h&amp;h&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}=&#92;dfrac{h^2}{2}&#92;begin{bmatrix}1&amp;1&amp;-1&#92;&#92;1&amp;1&amp;-1&#92;&#92;-1&amp;-1&amp;1&#92;end{bmatrix}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}$$</p>
<p>The first ($$x$$) component of the second-order error vector is:</p>
<p>$$&#92;dfrac{1}{4h}&#92;dfrac{h^2}{2}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;cdot&#92;left(&#92;begin{bmatrix}1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;end{bmatrix}+&#92;begin{bmatrix}1&amp;-1&amp;-1&#92;&#92;-1&amp;1&amp;1&#92;&#92;-1&amp;1&amp;1&#92;end{bmatrix}-&#92;begin{bmatrix}1&amp;-1&amp;1&#92;&#92;-1&amp;1&amp;-1&#92;&#92;1&amp;-1&amp;1&#92;end{bmatrix}-&#92;begin{bmatrix}1&amp;1&amp;-1&#92;&#92;1&amp;1&amp;-1&#92;&#92;-1&amp;-1&amp;1&#92;end{bmatrix}&#92;right)=&#92;dfrac{h}{8}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;cdot&#92;begin{bmatrix}0&amp;0&amp;0&#92;&#92;0&amp;0&amp;4&#92;&#92;0&amp;4&amp;0&#92;end{bmatrix}=f''&#95;{yz}$$</p>
<p>Wait, the second order error is not zero! Its $$x$$-component is $$&#92;dfrac{&#92;partial^2 f}{&#92;partial y &#92;partial z}$$. One my guess, is its $$y$$-component $$&#92;dfrac{&#92;partial^2 f}{&#92;partial x &#92;partial z}$$ and $$z$$-component $$&#92;dfrac{&#92;partial^2 f}{&#92;partial x &#92;partial y}$$?</p>
<p>$$&#92;dfrac{h}{8}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;cdot&#92;left(&#92;begin{bmatrix}1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;end{bmatrix}-&#92;begin{bmatrix}1&amp;-1&amp;-1&#92;&#92;-1&amp;1&amp;1&#92;&#92;-1&amp;1&amp;1&#92;end{bmatrix}+&#92;begin{bmatrix}1&amp;-1&amp;1&#92;&#92;-1&amp;1&amp;-1&#92;&#92;1&amp;-1&amp;1&#92;end{bmatrix}-&#92;begin{bmatrix}1&amp;1&amp;-1&#92;&#92;1&amp;1&amp;-1&#92;&#92;-1&amp;-1&amp;1&#92;end{bmatrix}&#92;right)=&#92;dfrac{h}{8}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;cdot&#92;begin{bmatrix}0&amp;0&amp;4&#92;&#92;0&amp;0&amp;0&#92;&#92;4&amp;0&amp;0&#92;end{bmatrix}=f''&#95;{xz}$$</p>
<p>$$&#92;dfrac{h}{8}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;cdot&#92;left(&#92;begin{bmatrix}1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;&#92;1&amp;1&amp;1&#92;end{bmatrix}-&#92;begin{bmatrix}1&amp;-1&amp;-1&#92;&#92;-1&amp;1&amp;1&#92;&#92;-1&amp;1&amp;1&#92;end{bmatrix}-&#92;begin{bmatrix}1&amp;-1&amp;1&#92;&#92;-1&amp;1&amp;-1&#92;&#92;1&amp;-1&amp;1&#92;end{bmatrix}+&#92;begin{bmatrix}1&amp;1&amp;-1&#92;&#92;1&amp;1&amp;-1&#92;&#92;-1&amp;-1&amp;1&#92;end{bmatrix}&#92;right)=&#92;dfrac{h}{8}&#92;cdot&#92;dfrac{&#92;partial^2 f}{&#92;partial p^2}&#92;cdot&#92;begin{bmatrix}0&amp;4&amp;0&#92;&#92;4&amp;0&amp;0&#92;&#92;0&amp;0&amp;0&#92;end{bmatrix}=f''&#95;{xy}$$</p>
<p>The answer is yes. Which means, the tetrahedron method does not have a second-order accuracy. Its error is approximately $$h&#92;begin{pmatrix}&#92;dfrac{&#92;partial^2 f}{&#92;partial y &#92;partial z},&#92;dfrac{&#92;partial^2 f}{&#92;partial x &#92;partial z},&#92;dfrac{&#92;partial^2 f}{&#92;partial x &#92;partial y}&#92;end{pmatrix}$$, which is proportional to $$h$$.</p>
<p>The tetrahedron method and forward differentiation have the same order of accuracy. The tetrahedron method needs four samples, while forward differentiation only require three samples if the point is already evaluated. But why is the tetrahedron method still widely used in computer graphics? In most cases, the tetrahedron method produces a better visual result than forward differentiation. I guess it's because forward differentiation has a shift like it is actually calculating the derivative at the middle of the four samples, while the tetrahedron method does not. Also, for a sphere-shaped scalar field, the off-diagonal terms of the Hessian matrix is almost zero, it is more accurate to eliminate the diagonal terms that involves higher error.</p>
<p>A comparison of forward differentiation (2nd from the left), central difference (3rd), and tetrahedron method (last) is followed:</p>
<iframe width="640" height="360" frameborder="0" src="https://www.shadertoy.com/embed/3lfyDl?gui=true&t=0&paused=true&muted=true" allowfullscreen></iframe>

<p>This demo compares numerical gradient with analytical gradient. Red color indicates the error in the direction of gradient, and blue color indicates the error in the magnitude of gradient. A higher color saturation indicates a higher error. Drag mouse up to increase numerical differentiation step $$h$$ and down to decrease $$h$$.</p>
        </div>
    </div>

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
            id="katex-auto-render"></script>
    <script>
        document.getElementById("katex-auto-render").onload = function () {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: false }
                ]
            });
        }
    </script>
</body>
</html>
